{
    "schemaVersion": "2.2",
    "description": "Run a script on Linux instance to perform for disk investigation.",
    "parameters": {
        "s3Location": {
            "type": "String",
            "description": "S3 location to load the results into bucket",
            "default": "aws s3 cp - s3://{3}/{1}/{2}/{0}/memory/{0}.raw"
        },
        "attachedVolumeId": {
            "type": "String",
            "description": "(Required)Volume Details.",
            "default": "attachedVolumeId"
        },
        "attachedVolumeMountInfo": {
            "type": "String",
            "description": "(Required)Volume Details.",
            "default": "Volumedetails"
        },
        "AccessKeyId": {
            "type": "String",
            "description": "(Required) AccesskeyID to store evidence details in S3.",
            "default": "Access key ID"
        },
        "SecretAccessKey": {
            "type": "String",
            "description": "(Required) SecretAccessKey to store evidence details in S3",
            "default": "secret access key"
        },
        "SessionToken": {
            "type": "String",
            "description": "(Required) SessionToken to store evidence details in S3.",
            "default": "session Token"
        },
        "forensicID": {
            "type": "String",
            "description": "(Required) forensicID of memory acquisition",
            "default": "session Token"
        },
        "Region": {
            "type": "String",
            "description": "(Required) Region details.",
            "default": "ap-southeast-2"
        },
        "ParserID": {
            "type": "String",
            "description": "The expression is a comma separated string where each element is a preset, parser or plugin name. To view run 'log2timeline.py --parsers list' or '--info' to list available presets, parsers and plugins.",
            "default": "linux"
        },
        "TargetVolume": {
            "type": "String",
            "description": "Target all or specific volumes for forensic analysis. Multiple volumes can be defined as: '1,3,5' (a list of comma separated values). Ranges and lists can also be combined as: 1,3..5. The first volume is 1. All volumes can be specified with: all",
            "default": "all"
        }
    },
    "mainSteps": [
        {
            "action": "aws:runShellScript",
            "name": "runCommands",
            "precondition": {
                "StringEquals": ["platformType", "Linux"]
            },
            "inputs": {
                "timeoutSeconds": "4000",
                "runCommand": [
                    "#!/bin/bash",
                    "cd /tmp",
                    "mkdir forensic-analysis",
                    "cd forensic-analysis",
                    "instance=$(echo {{s3Location}} | sed -n 's/.*\\(i-[a-z0-9]{17\\}\\).*/\\1/p')",
                    "mkdir -p /tmp/forensic-analysis/data/{{forensicID}}_evidence_dir/$instance",
                    "mkdir -p /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance",
                    "cd data/{{forensicID}}_evidence_dir/$instance",
                    "export AWS_ACCESS_KEY_ID={{AccessKeyId}}",
                    "export AWS_SECRET_ACCESS_KEY={{SecretAccessKey}}",
                    "export AWS_SESSION_TOKEN={{SessionToken}}",
                    "export AWS_DEFAULT_REGION={{Region}}",
                    "pip3 install --upgrade awscli",
                    "aws configure set default.s3.max_concurrent_requests 100",
                    "ls",
                    "cd /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance",
                    "sudo yum install docker -y",
                    "sudo systemctl start docker",
                    "sudo docker run -v {{attachedVolumeMountInfo}}/:{{attachedVolumeMountInfo}} log2timeline/plaso log2timeline -u --status_view none --partitions all --volumes {{TargetVolume}} --hashers md5 --hasher_file_size_limit 1073741824 --logfile {{attachedVolumeMountInfo}}/log2timeline-{{attachedVolumeId}}.log.gz --parser {{ParserID}} --storage_file {{attachedVolumeMountInfo}}/log2timeline_{{attachedVolumeId}}_{{ParserID}}.plaso {{attachedVolumeMountInfo}}",
                    "sudo docker run -v {{attachedVolumeMountInfo}}/:{{attachedVolumeMountInfo}} log2timeline/plaso psort -u --status_view none -o dynamic --dynamic-time --fields datetime,date,time,timezone,macb,type,source,sourcetype,user,hostname,description,filename,inode,md5_hash -w {{attachedVolumeMountInfo}}/log2timeline_{{attachedVolumeId}}_output.csv {{attachedVolumeMountInfo}}/log2timeline_{{attachedVolumeId}}_{{ParserID}}.plaso",
                    "ls -ltr {{attachedVolumeMountInfo}}",
                    "mv {{attachedVolumeMountInfo}}/log2timeline_{{attachedVolumeId}}_{{ParserID}}.plaso /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance",
                    "mv {{attachedVolumeMountInfo}}/log2timeline_{{attachedVolumeId}}_output.csv /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance",
                    "ls -ltr",
                    "sha256sum /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance/log2timeline_{{attachedVolumeId}}_output.csv > log2timeline_{{attachedVolumeId}}_output_sha256.txt ",
                    "ls",
                    "aws s3 cp /tmp/forensic-analysis/data/{{forensicID}}_working_dir/$instance {{s3Location}}  --recursive",
                    "echo s3 cp complete",
                    "ls -ltr",
                    "exit 0;"
                ]
            }
        }
    ]
}
